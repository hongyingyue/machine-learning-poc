# LLM Inference

## [sglang](https://github.com/sgl-project/sglang)
- use triton

## [vllm](https://github.com/vllm-project/vllm)
- concurrent request

## General QA
- warm up service
- 
